# -*- coding: utf-8 -*-
import os
import sys
import gradio as gr
import google.generativeai as genai
# 'types' c√≥ th·ªÉ kh√¥ng c·∫ßn thi·∫øt n·ªØa n·∫øu ch·ªâ d√πng Client API v√† dictionary config
# from google.generativeai import types
from google.api_core import exceptions as google_exceptions
import traceback

# ================= C·∫§U H√åNH API KEY XOAY V√íNG =================
API_KEYS = [
    "AIzaSyBybfBSDLx39DdnZbHyLbd21tQAdfHtbeE",
    "AIzaSyCFCj6v8hD49BICKhnHLEpP5o_Wn7hrJgg",
    "AIzaSyBxCiE0J23G9jRJvAX7Q9CmPP2BTfTUP4o",
    "AIzaSyDkeIgLhVdtCKkP3O-E6NtddP1DCdsQJO8",
]
API_KEYS = [key for key in API_KEYS if key and not key.startswith("YOUR_")]
current_key_index = 0

def rotate_api_key():
    global current_key_index
    if not API_KEYS: return None
    current_key_index = (current_key_index + 1) % len(API_KEYS)
    print(f"[INFO] Chuy·ªÉn sang s·ª≠ d·ª•ng API Key #{current_key_index + 1}")
    return API_KEYS[current_key_index]

# ================= MODEL V√Ä C·∫§U H√åNH =================
MODEL_NAME = "gemini-2.5-pro-exp-03-25"
print(f"[INFO] S·ª≠ d·ª•ng model: {MODEL_NAME}")

# --- X√ìA KH·ªêI ƒê·ªäNH NGHƒ®A generation_config TO√ÄN C·ª§C ---
# Kh·ªëi code n√†y ƒë√£ b·ªã x√≥a v√¨ g√¢y l·ªói AttributeError
# generation_config = types.GenerationConfig(
#     thinking_config=types.ThinkingConfig(
#         thinking_budget=0,
#     ),
#     response_mime_type="text/plain",
# )
# --- K·∫æT TH√öC X√ìA ---

# ================= H√ÄM X·ª¨ L√ù L·ªñI API (Gi·ªØ nguy√™n) =================
def format_api_error(e, key_index):
    error_message = str(e)
    error_type = type(e).__name__
    print(f"[ERROR][Key #{key_index + 1}] L·ªói khi g·ªçi API: {error_type} - {error_message}")
    traceback.print_exc() # In stack trace ƒë·ªÉ debug

    # Ph√¢n lo·∫°i l·ªói ƒë·ªÉ quy·∫øt ƒë·ªãnh c√≥ n√™n xoay key hay kh√¥ng
    is_key_related_error = False
    user_friendly_message = f"‚ùå L·ªói v·ªõi Key #{key_index + 1} ({error_type})"

    if isinstance(e, google_exceptions.PermissionDenied):
        if "API key not valid" in error_message or "API_KEY_INVALID" in error_message:
            user_friendly_message = f"‚ùå L·ªói: API Key #{key_index + 1} kh√¥ng h·ª£p l·ªá ho·∫∑c ƒë√£ b·ªã v√¥ hi·ªáu h√≥a (PermissionDenied)."
            is_key_related_error = True
        else: # L·ªói quy·ªÅn truy c·∫≠p model
            user_friendly_message = f"‚ùå L·ªói: T·ª´ ch·ªëi quy·ªÅn truy c·∫≠p (Permission Denied) cho model '{MODEL_NAME}' v·ªõi Key #{key_index + 1}. Key c√≥ th·ªÉ kh√¥ng c√≥ quy·ªÅn s·ª≠ d·ª•ng model n√†y ho·∫∑c ch∆∞a b·∫≠t 'Generative Language API'."
            is_key_related_error = True # Th∆∞·ªùng l√† v·∫•n ƒë·ªÅ key/quy·ªÅn
    elif isinstance(e, google_exceptions.InvalidArgument):
         if "API key not valid" in error_message:
            user_friendly_message = f"‚ùå L·ªói: API Key #{key_index + 1} kh√¥ng h·ª£p l·ªá (InvalidArgument)."
            is_key_related_error = True
         elif "user location is not supported" in error_message.lower():
             user_friendly_message = f"‚ùå L·ªói: Khu v·ª±c c·ªßa b·∫°n kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£ ƒë·ªÉ s·ª≠ d·ª•ng API n√†y (InvalidArgument - Location)."
             is_key_related_error = False # L·ªói n√†y th∆∞·ªùng kh√¥ng ph·∫£i do key
         elif "invalid content" in error_message.lower():
             user_friendly_message = f"‚ùå L·ªói: D·ªØ li·ªáu g·ª≠i ƒëi kh√¥ng h·ª£p l·ªá (InvalidArgument). C√≥ th·ªÉ do l·ªãch s·ª≠ chat b·ªã l·ªói."
             is_key_related_error = False # L·ªói n√†y kh√¥ng ph·∫£i do key
         else:
            user_friendly_message = f"‚ùå L·ªói: Tham s·ªë kh√¥ng h·ª£p l·ªá (InvalidArgument) v·ªõi Key #{key_index + 1}: {error_message}"
            # C√≥ th·ªÉ do key, c√≥ th·ªÉ kh√¥ng, t·∫°m coi l√† kh√¥ng ƒë·ªÉ tr√°nh xoay v√≤ng v√¥ √≠ch
    elif isinstance(e, google_exceptions.NotFound):
         user_friendly_message = f"‚ùå L·ªói: Model '{MODEL_NAME}' kh√¥ng t√¨m th·∫•y ho·∫∑c kh√¥ng t·ªìn t·∫°i v·ªõi API key #{key_index + 1}."
         is_key_related_error = True # C√≥ th·ªÉ do key kh√¥ng c√≥ quy·ªÅn truy c·∫≠p model n√†y
    elif isinstance(e, google_exceptions.ResourceExhausted):
         user_friendly_message = f"‚ùå L·ªói: Key #{key_index + 1} ƒë√£ v∆∞·ª£t qu√° H·∫°n ng·∫°ch API (Quota) ho·∫∑c T√†i nguy√™n ƒë√£ c·∫°n ki·ªát (429)."
         is_key_related_error = True
    elif isinstance(e, google_exceptions.DeadlineExceeded):
         user_friendly_message = f"‚ùå L·ªói: Y√™u c·∫ßu v·ªõi Key #{key_index + 1} v∆∞·ª£t qu√° th·ªùi gian ch·ªù (Timeout/Deadline Exceeded/504)."
    elif isinstance(e, (genai.types.BlockedPromptException, genai.types.StopCandidateException)):
         # L∆∞u √Ω: genai.types v·∫´n c√≥ th·ªÉ c·∫ßn cho Exception n√†y
         user_friendly_message = f"‚ö†Ô∏è Y√™u c·∫ßu ho·∫∑c ph·∫£n h·ªìi b·ªã ch·∫∑n b·ªüi b·ªô l·ªçc an to√†n."
    else: # C√°c l·ªói kh√°c
         user_friendly_message = f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh v·ªõi Key #{key_index + 1} ({error_type}): {error_message}"

    return user_friendly_message, is_key_related_error

# H√†m ti·ªán √≠ch ƒë·ªÉ th√™m l·ªói v√†o l·ªãch s·ª≠ chat (Gi·ªØ nguy√™n)
def append_error_to_history(error_msg, message, chat_history_state):
    if isinstance(chat_history_state, list):
        if not chat_history_state or chat_history_state[-1] != [message, error_msg]:
             chat_history_state.append([message, error_msg])
    else:
        chat_history_state = [[message, error_msg]]
    return "", chat_history_state, chat_history_state

# ================= H√ÄM CALLBACK CH√çNH C·ª¶A GRADIO =================
def respond(message, chat_history_state):
    global current_key_index
    from google.generativeai import types as genai_types # Import c·ª•c b·ªô n·∫øu c·∫ßn cho Exception

    if not API_KEYS:
        error_msg = "‚ùå L·ªói c·∫•u h√¨nh: Danh s√°ch API Key tr·ªëng!"
        return append_error_to_history(error_msg, message, chat_history_state)
    if not message or message.strip() == "":
        # Kh√¥ng l√†m g√¨ n·∫øu message tr·ªëng
        return "", chat_history_state, chat_history_state

    current_chat_history = list(chat_history_state)
    contents = []
    for user_msg, model_msg in current_chat_history:
        # C·∫ßn import Part ·ªü ƒë√¢y n·∫øu ch∆∞a import types ·ªü tr√™n c√πng
        from google.generativeai.types import Part
        if user_msg and isinstance(user_msg, str):
            contents.append({'role': 'user', 'parts': [Part.from_text(text=user_msg)]})
        if model_msg and isinstance(model_msg, str) and not model_msg.startswith("‚ùå") and not model_msg.startswith("‚ö†Ô∏è"):
            contents.append({'role': 'model', 'parts': [Part.from_text(text=model_msg)]})
    # Th√™m tin nh·∫Øn m·ªõi nh·∫•t (c≈©ng c·∫ßn Part)
    from google.generativeai.types import Part
    contents.append({'role': 'user', 'parts': [Part.from_text(text=message)]})

    print(f"[INFO] L·ªãch s·ª≠ g·ª≠i ƒëi ('contents' length): {len(contents)}")
    print(f"[INFO] Prompt m·ªõi: '{message[:100]}...'")

    initial_key_index = current_key_index
    for attempt in range(len(API_KEYS)):
        active_key = API_KEYS[current_key_index]
        print(f"[INFO] ƒêang th·ª≠ v·ªõi API Key #{current_key_index + 1}...")
        try:
            client = genai.Client(api_key=active_key)

            # --- S·ª¨A L·ªñI T·∫†I ƒê√ÇY ---
            # Truy·ªÅn c·∫•u h√¨nh tr·ª±c ti·∫øp d∆∞·ªõi d·∫°ng dictionary
            response_stream = client.models.generate_content_stream(
                model=f"models/{MODEL_NAME}",
                contents=contents,
                generation_config={ # <<< TRUY·ªÄN DICTIONARY
                    "thinking_config": {
                        "thinking_budget": 0,
                    },
                    "response_mime_type": "text/plain",
                    # B·∫°n c√≥ th·ªÉ th√™m c√°c tham s·ªë kh√°c c·ªßa GenerationConfig v√†o ƒë√¢y
                    # v√≠ d·ª•: "temperature": 0.7, "max_output_tokens": 2048
                },
                stream=True,
            )
            # --- K·∫æT TH√öC S·ª¨A L·ªñI ---

            current_chat_history.append([message, ""])
            full_response_text = ""
            thinking_active = False
            for chunk in response_stream:
                if chunk.thinking_state:
                    if not thinking_active:
                        print("[INFO] ZyraX is ThinKing...")
                        thinking_active = True
                        current_chat_history[-1][1] = full_response_text + "..."
                        yield "", current_chat_history, current_chat_history
                    continue
                chunk_text = getattr(chunk, 'text', '')
                if chunk_text:
                    if thinking_active:
                        current_chat_history[-1][1] = full_response_text
                        thinking_active = False
                    full_response_text += chunk_text
                    current_chat_history[-1][1] = full_response_text
                    yield "", current_chat_history, current_chat_history

                # X·ª≠ l√Ω Block/Finish Reason (c·∫ßn ki·ªÉm tra l·∫°i n·∫øu c·∫ßn genai_types)
                block_reason = getattr(getattr(chunk, 'prompt_feedback', None), 'block_reason', None)
                finish_reason = None
                if hasattr(chunk, 'candidates') and chunk.candidates:
                     finish_reason = getattr(chunk.candidates[0], 'finish_reason', None)
                reason_text = ""
                should_stop = False
                # C√≥ th·ªÉ c·∫ßn ki·ªÉm tra ki·ªÉu enum t·ª´ th∆∞ vi·ªán g·ªëc n·∫øu d√πng tr·ª±c ti·∫øp
                # V√≠ d·ª•: from google.ai.generativelanguage import SafetySetting
                if block_reason: # Gi·∫£ s·ª≠ block_reason tr·∫£ v·ªÅ gi√° tr·ªã c√≥ th·ªÉ ƒë√°nh gi√° l√† True
                    reason_text, should_stop = f"Y√™u c·∫ßu/Ph·∫£n h·ªìi b·ªã ch·∫∑n ({block_reason})", True
                elif finish_reason and finish_reason != 'STOP': # Gi·∫£ s·ª≠ finish_reason l√† string
                    reason_text, should_stop = f"Ph·∫£n h·ªìi b·ªã d·ª´ng s·ªõm ({finish_reason})", True

                if reason_text:
                    print(f"[WARN] {reason_text}")
                    warning_msg = f"\n‚ö†Ô∏è ({reason_text})"
                    if not current_chat_history[-1][1] or current_chat_history[-1][1].isspace():
                         current_chat_history[-1][1] = warning_msg.strip()
                    elif warning_msg not in current_chat_history[-1][1]:
                         current_chat_history[-1][1] += warning_msg
                    yield "", current_chat_history, current_chat_history
                    if should_stop:
                        print("[INFO] D·ª´ng x·ª≠ l√Ω do block/finish reason.")
                        return

            print(f"[OK] Ho√†n th√†nh streaming v·ªõi Key #{current_key_index + 1}.")
            return

        except Exception as e:
            # Check if genai_types is needed for exception handling
            if isinstance(e, (genai_types.BlockedPromptException, genai_types.StopCandidateException)):
                 error_msg, is_key_error = format_api_error(e, current_key_index) # G·ªçi h√†m c≈©
                 # L·ªói n√†y kh√¥ng li√™n quan ƒë·∫øn key
                 print(f"[ERROR] G·∫∑p l·ªói ch·∫∑n n·ªôi dung, d·ª´ng th·ª≠.")
                 return append_error_to_history(error_msg, message, current_chat_history)
            else:
                # X·ª≠ l√Ω c√°c l·ªói kh√°c nh∆∞ c≈©
                error_msg, is_key_error = format_api_error(e, current_key_index)
                if is_key_error:
                    rotate_api_key()
                    if current_key_index == initial_key_index and attempt == len(API_KEYS) - 1:
                        print("[ERROR] ƒê√£ th·ª≠ t·∫•t c·∫£ API Key nh∆∞ng ƒë·ªÅu th·∫•t b·∫°i.")
                        final_error_msg = f"‚ùå ƒê√£ th·ª≠ t·∫•t c·∫£ {len(API_KEYS)} API Key. L·ªói cu·ªëi: {error_msg}"
                        return append_error_to_history(final_error_msg, message, current_chat_history)
                    continue
                else:
                    print(f"[ERROR] L·ªói kh√¥ng li√™n quan key, d·ª´ng th·ª≠.")
                    return append_error_to_history(error_msg, message, current_chat_history)

    print("[ERROR] Kh√¥ng th·ªÉ ho√†n th√†nh y√™u c·∫ßu sau khi th·ª≠ t·∫•t c·∫£ API Key.")
    final_error_msg = f"‚ùå ƒê√£ th·ª≠ t·∫•t c·∫£ {len(API_KEYS)} API Key nh∆∞ng kh√¥ng th√†nh c√¥ng."
    return append_error_to_history(final_error_msg, message, current_chat_history)


# ================= GIAO DI·ªÜN GRADIO (Gi·ªØ nguy√™n) =================
custom_theme = gr.themes.Soft(
    primary_hue="emerald", secondary_hue="gray",
).set(
    button_primary_background_fill="*primary_500",
    button_primary_background_fill_hover="*primary_400",
    chatbot_code_background_color="*primary_50",
)

with gr.Blocks(theme=custom_theme, title="ZyRa X - Gemini Pro (Thinking)") as demo:
    with gr.Row():
        gr.HTML("""
            <div style="text-align: center; width: 100%;">
                <h1 style="font-family: 'Segoe UI', sans-serif; color: #2ecc71;">
                    <img src="https://i.ibb.co/3yRk2L2/ai-icon.png" style="height: 40px; vertical-align: middle; margin-right: 10px;">ZyRa X
                </h1>
                <p style="color: #7f8c8d;">Model: Gemini 2.5 Pro Exp (Thinking Enabled) - Multi API Key</p>
            </div>
        """)
    chatbot = gr.Chatbot(
        label="L·ªãch s·ª≠ Chat", height=600,
        avatar_images=("https://i.ibb.co/rdZC7LZ/user-avatar.png", "https://i.ibb.co/3yRk2L2/ai-icon.png"),
        bubble_full_width=False, render_markdown=True, show_label=False,
        latex_delimiters=[{"left": "$$", "right": "$$", "display": True}, {"left": "$", "right": "$", "display": False},
                          {"left": "\\[", "right": "\\]", "display": True}, {"left": "\\(", "right": "\\)", "display": False}]
    )
    chat_history_state = gr.State([])
    key_index_state = gr.State(current_key_index)
    with gr.Row():
        with gr.Column(scale=8):
            msg = gr.Textbox(placeholder="Nh·∫≠p c√¢u h·ªèi ho·∫∑c y√™u c·∫ßu c·ªßa b·∫°n ·ªü ƒë√¢y...", lines=2, max_lines=5, label="Tin nh·∫Øn", container=False, show_label=False)
        with gr.Column(scale=1, min_width=80):
            send_btn = gr.Button("G·ª≠i", variant="primary", size="sm")
        with gr.Column(scale=1, min_width=80):
            clear_btn = gr.Button("üóëÔ∏è X√≥a", variant="secondary", size="sm")
    with gr.Accordion("‚ìò Tr·∫°ng th√°i API", open=False):
         key_status_display = gr.Markdown(f"S·∫µn s√†ng s·ª≠ d·ª•ng Key #{current_key_index + 1} / {len(API_KEYS) if API_KEYS else 0}", elem_id="key-status")

    def submit_message(message, history, key_idx_state):
        # S·ª≠ d·ª•ng yield from ƒë·ªÉ ch·∫°y respond v√† c·∫≠p nh·∫≠t msg, chatbot, history
        yield from respond(message, history)
        # Sau khi respond ch·∫°y xong, c·∫≠p nh·∫≠t tr·∫°ng th√°i key
        new_key_idx = current_key_index
        key_info = f"ƒêang d√πng Key #{new_key_idx + 1} / {len(API_KEYS) if API_KEYS else 0}"
        yield gr.Markdown(value=key_info) # Tr·∫£ v·ªÅ update cho key_status_display

    msg.submit(submit_message, inputs=[msg, chat_history_state, key_index_state], outputs=[msg, chatbot, chat_history_state, key_status_display])
    send_btn.click(submit_message, inputs=[msg, chat_history_state, key_index_state], outputs=[msg, chatbot, chat_history_state, key_status_display])

    def clear_chat_func():
        global current_key_index
        key_info = f"ƒêang d√πng Key #{current_key_index + 1} / {len(API_KEYS) if API_KEYS else 0}"
        return "", [], gr.Markdown(value=key_info)

    clear_btn.click(clear_chat_func, outputs=[msg, chatbot, chat_history_state, key_status_display], queue=False)

# ================= CH·∫†Y ·ª®NG D·ª§NG (Gi·ªØ nguy√™n) =================
if __name__ == "__main__":
    if not API_KEYS:
        print("\n" + "="*50)
        print("‚ö†Ô∏è C·∫¢NH B√ÅO: Kh√¥ng t√¨m th·∫•y API Key h·ª£p l·ªá n√†o.")
        print("="*50 + "\n")
    print("ƒêang kh·ªüi ch·∫°y Gradio UI...")
    demo.queue().launch(
        server_name='0.0.0.0', server_port=int(os.environ.get('PORT', 7860)),
        share=False, debug=False, favicon_path="https://i.ibb.co/3yRk2L2/ai-icon.png"
    )
    print("Gradio UI ƒë√£ kh·ªüi ch·∫°y.")
