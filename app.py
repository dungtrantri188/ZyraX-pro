# -*- coding: utf-8 -*-
import os
import sys
import time  # <-- ThÃªm import nÃ y
import gradio as gr
import google.generativeai as genai
from google.api_core import exceptions as google_exceptions

# --- API Key Ä‘Æ°á»£c Ä‘áº·t trá»±c tiáº¿p theo yÃªu cáº§u ---
# LÆ°u Ã½: Key nÃ y Ä‘Ã£ bÃ¡o lá»—i khÃ´ng há»£p lá»‡ á»Ÿ láº§n kiá»ƒm tra trÆ°á»›c.
# Náº¿u nÃ³ váº«n khÃ´ng há»£p lá»‡, á»©ng dá»¥ng sáº½ bÃ¡o lá»—i trong chat.
API_KEY = "AIzaSyBybfBSDLx39DdnZbHyLbd21tQAdfHtbeE" # VáºªN LÃ€ Rá»¦I RO Báº¢O Máº¬T Lá»šN

genai_configured = False
# 1) Kiá»ƒm tra vÃ  cáº¥u hÃ¬nh API Key tá»« code (Giá»¯ nguyÃªn)
if not API_KEY:
    print("[ERROR] API Key bá»‹ thiáº¿u trong code.]")
else:
    print("[INFO] API Key Ä‘Æ°á»£c táº£i trá»±c tiáº¿p tá»« code.")
    print("Äang cáº¥u hÃ¬nh Google AI...")
    try:
        genai.configure(api_key=API_KEY)
        genai_configured = True
        print("[OK] Google AI Ä‘Ã£ Ä‘Æ°á»£c cáº¥u hÃ¬nh thÃ nh cÃ´ng (cÃº phÃ¡p).")
    except Exception as e:
        print(f"[ERROR] KhÃ´ng thá»ƒ cáº¥u hÃ¬nh Google AI ngay cáº£ vá»›i cÃº phÃ¡p: {e}")
        genai_configured = False

# 2) Model vÃ  HÃ m trá»£ giÃºp Ä‘á»‹nh dáº¡ng lá»—i (Giá»¯ nguyÃªn)
MODEL_NAME_CHAT = "gemini-2.5-flash-preview-04-17"
print(f"Sá»­ dá»¥ng model chat: {MODEL_NAME_CHAT}")

def format_api_error(e):
    # ... (HÃ m format_api_error giá»¯ nguyÃªn nhÆ° phiÃªn báº£n trÆ°á»›c) ...
    error_message = str(e)
    error_type = type(e).__name__
    print(f"[ERROR] Lá»—i khi gá»i API: {error_type} - {error_message}") # Log lá»—i

    if isinstance(e, google_exceptions.PermissionDenied):
        if "API key not valid" in error_message or "API_KEY_INVALID" in error_message:
             return "âŒ Lá»—i: API Key Ä‘Æ°á»£c cáº¥u hÃ¬nh nhÆ°ng Google tá»« chá»‘i khi sá»­ dá»¥ng (API_KEY_INVALID). CÃ³ thá»ƒ key Ä‘Ã£ bá»‹ vÃ´ hiá»‡u hÃ³a."
        else: # Lá»—i quyá»n truy cáº­p model
             return f"âŒ Lá»—i: Tá»« chá»‘i quyá»n truy cáº­p (Permission Denied) cho model '{MODEL_NAME_CHAT}'. API key cá»§a báº¡n cÃ³ thá»ƒ khÃ´ng cÃ³ quyá»n sá»­ dá»¥ng model nÃ y hoáº·c chÆ°a báº­t 'Generative Language API' trong Google Cloud."
    elif isinstance(e, google_exceptions.InvalidArgument) and "API key not valid" in error_message:
        return "âŒ Lá»—i: API Key khÃ´ng há»£p lá»‡ (InvalidArgument). Key cung cáº¥p khÃ´ng Ä‘Ãºng hoáº·c Ä‘Ã£ bá»‹ vÃ´ hiá»‡u hÃ³a."
    elif isinstance(e, google_exceptions.NotFound):
         return f"âŒ Lá»—i: Model '{MODEL_NAME_CHAT}' khÃ´ng tÃ¬m tháº¥y hoáº·c khÃ´ng tá»“n táº¡i vá»›i API key cá»§a báº¡n."
    elif isinstance(e, google_exceptions.ResourceExhausted):
         return "âŒ Lá»—i: ÄÃ£ vÆ°á»£t quÃ¡ Háº¡n ngáº¡ch API (Quota) hoáº·c TÃ i nguyÃªn Ä‘Ã£ cáº¡n kiá»‡t (429). Vui lÃ²ng thá»­ láº¡i sau."
    elif isinstance(e, google_exceptions.DeadlineExceeded):
         return "âŒ Lá»—i: YÃªu cáº§u vÆ°á»£t quÃ¡ thá»i gian chá» (Timeout/Deadline Exceeded/504)."
    elif isinstance(e, AttributeError) and "start_chat" in error_message:
         return f"âŒ Lá»—i: Model '{MODEL_NAME_CHAT}' cÃ³ thá»ƒ khÃ´ng há»— trá»£ phÆ°Æ¡ng thá»©c `start_chat`."
    else: # CÃ¡c lá»—i khÃ¡c
         return f"âŒ Lá»—i khi gá»i AI ({error_type}): {error_message}"

# 3) HÃ m callback Gradio (Sá»­a Ä‘á»•i vÃ²ng láº·p stream Ä‘á»ƒ lÃ m cháº­m tá»‘c Ä‘á»™)
def respond(message, chat_history_state):
    if not genai_configured:
        error_msg = "âŒ Lá»—i: Google AI chÆ°a Ä‘Æ°á»£c cáº¥u hÃ¬nh Ä‘Ãºng cÃ¡ch (API Key cÃ³ váº¥n Ä‘á» hoáº·c cáº¥u hÃ¬nh tháº¥t báº¡i)."
        if isinstance(chat_history_state, list):
             chat_history_state.append([message, error_msg])
        else:
             chat_history_state = [[message, error_msg]]
        return "", chat_history_state, chat_history_state

    current_chat_history = list(chat_history_state)
    gemini_history = []
    for user_msg, model_msg in current_chat_history:
        if user_msg and isinstance(user_msg, str):
             gemini_history.append({'role': 'user', 'parts': [user_msg]})
        if model_msg and isinstance(model_msg, str) and not model_msg.startswith("âŒ") and not model_msg.startswith("âš ï¸"):
             gemini_history.append({'role': 'model', 'parts': [model_msg]})

    print(f"Lá»‹ch sá»­ gá»­i tá»›i Gemini: {gemini_history}")
    print(f"Prompt má»›i: '{message[:70]}...'")

    try:
        model = genai.GenerativeModel(MODEL_NAME_CHAT)
        chat = model.start_chat(history=gemini_history)
        response = chat.send_message(message, stream=True)

        current_chat_history.append([message, ""])
        full_response_text = ""

        # --- THAY Äá»”I Báº®T Äáº¦U Tá»ª ÄÃ‚Y ---
        for chunk in response:
            try:
                chunk_text = getattr(chunk, 'text', '')
                if chunk_text:
                    # Thay vÃ¬ thÃªm cáº£ chunk, thÃªm tá»«ng kÃ½ tá»± vÃ  yield
                    for char in chunk_text:
                        full_response_text += char
                        current_chat_history[-1][1] = full_response_text
                        yield "", current_chat_history, current_chat_history
                        # ThÃªm Ä‘á»™ trá»… nhá» sau má»—i kÃ½ tá»±
                        time.sleep(0.02) # <-- Äiá»u chá»‰nh giÃ¡ trá»‹ nÃ y Ä‘á»ƒ thay Ä‘á»•i tá»‘c Ä‘á»™ (0.02 giÃ¢y = 20ms)
                                        # TÄƒng lÃªn (vÃ­ dá»¥ 0.05) Ä‘á»ƒ cháº­m hÆ¡n, giáº£m xuá»‘ng (vÃ­ dá»¥ 0.01) Ä‘á»ƒ nhanh hÆ¡n.
                else:
                    # (Logic kiá»ƒm tra block/finish reason giá»¯ nguyÃªn)
                    block_reason = getattr(getattr(chunk, 'prompt_feedback', None), 'block_reason', None)
                    finish_reason = getattr(getattr(chunk.candidates[0], 'finish_reason', None)) if chunk.candidates else None
                    reason_text = ""
                    should_stop = False
                    if block_reason: reason_text, should_stop = f"YÃªu cáº§u/Pháº£n há»“i bá»‹ cháº·n ({block_reason})", True
                    elif finish_reason and finish_reason != 'STOP': reason_text, should_stop = f"Pháº£n há»“i bá»‹ dá»«ng ({finish_reason})", True

                    if reason_text:
                        print(f"[WARN] {reason_text}")
                        warning_msg = f"\nâš ï¸ ({reason_text})"
                        if not current_chat_history[-1][1] or current_chat_history[-1][1].isspace(): current_chat_history[-1][1] = warning_msg.strip()
                        elif warning_msg not in current_chat_history[-1][1]: current_chat_history[-1][1] += warning_msg
                        yield "", current_chat_history, current_chat_history
                        if should_stop: return

            except Exception as inner_e:
                print(f"[ERROR] Lá»—i khi xá»­ lÃ½ chunk stream: {type(inner_e).__name__} - {inner_e}")
                error_warning = f"\nâš ï¸ (Lá»—i khi xá»­ lÃ½ pháº§n tiáº¿p theo: {inner_e})"
                if error_warning not in current_chat_history[-1][1]:
                    current_chat_history[-1][1] += error_warning
                yield "", current_chat_history, current_chat_history
                return
        # --- THAY Äá»”I Káº¾T THÃšC Táº I ÄÃ‚Y ---

        print("[OK] Streaming hoÃ n táº¥t.")

    except Exception as e:
        error_msg = format_api_error(e)
        current_chat_history.append([message, error_msg])
        yield "", current_chat_history, current_chat_history


# 4) UI Gradio (Giá»¯ nguyÃªn)
with gr.Blocks(theme=gr.themes.Default()) as demo:
    gr.Markdown("## ZyRa X - táº¡o bá»Ÿi DÅ©ng")
    # gr.Markdown("ðŸš¨ **Cáº£nh bÃ¡o:** ... ", elem_classes="warning") # <-- ÄÃ£ xÃ³a dÃ²ng nÃ y

    chatbot = gr.Chatbot(
        label="Chatbot",
        height=500,
        bubble_full_width=False,
        type='tuples', # Chá»‰ Ä‘á»‹nh rÃµ rÃ ng
        # avatar_images=("user.png", "bot.png")
        render_markdown=True, # Äáº£m báº£o markdown rendering Ä‘Æ°á»£c báº­t (máº·c Ä‘á»‹nh lÃ  True)
        latex_delimiters=[
            { "left": "$$", "right": "$$", "display": True },
            { "left": "$", "right": "$", "display": False },
            # { "left": "\\[", "right": "\\]", "display": True },
            # { "left": "\\(", "right": "\\)", "display": False }
        ]
    )
    chat_history_state = gr.State(value=[])

    with gr.Row():
        msg = gr.Textbox(placeholder="Nháº­p cÃ¢u há»i cá»§a báº¡n...", label="Báº¡n", scale=4, container=False)
        send_btn = gr.Button("Gá»­i")

    clear_btn = gr.Button("ðŸ—‘ï¸ XÃ³a cuá»™c trÃ² chuyá»‡n")

    # --- Káº¿t ná»‘i sá»± kiá»‡n (Giá»¯ nguyÃªn) ---
    submit_event = msg.submit(respond, inputs=[msg, chat_history_state], outputs=[msg, chatbot, chat_history_state])
    click_event = send_btn.click(respond, inputs=[msg, chat_history_state], outputs=[msg, chatbot, chat_history_state])

    # HÃ m xÃ³a chat (Giá»¯ nguyÃªn)
    def clear_chat_func(): return "", [], []
    clear_btn.click(clear_chat_func, outputs=[msg, chatbot, chat_history_state], queue=False)

# 5) Cháº¡y á»©ng dá»¥ng (Giá»¯ nguyÃªn)
print("Äang khá»Ÿi cháº¡y Gradio UI...")
# Modified launch command to bind to 0.0.0.0 and use the PORT environment variable
demo.queue().launch(server_name='0.0.0.0', server_port=int(os.environ.get('PORT', 7860)), debug=False)
print("Gradio UI Ä‘Ã£ khá»Ÿi cháº¡y.")
