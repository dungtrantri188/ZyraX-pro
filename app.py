# -*- coding: utf-8 -*-
import os
import sys
import gradio as gr
import google.generativeai as genai
from google.api_core import exceptions as google_exceptions
import time # <--- ThÃªm thÆ° viá»‡n time

# --- API Key Ä‘Æ°á»£c Ä‘áº·t trá»±c tiáº¿p theo yÃªu cáº§u ---
# LÆ°u Ã½: Key nÃ y Ä‘Ã£ bÃ¡o lá»—i khÃ´ng há»£p lá»‡ á»Ÿ láº§n kiá»ƒm tra trÆ°á»›c.
# Náº¿u nÃ³ váº«n khÃ´ng há»£p lá»‡, á»©ng dá»¥ng sáº½ bÃ¡o lá»—i trong chat.
API_KEY = "AIzaSyBybfBSDLx39DdnZbHyLbd21tQAdfHtbeE"

genai_configured = False
# 1) Kiá»ƒm tra vÃ  cáº¥u hÃ¬nh API Key tá»« code
if not API_KEY:
    print("[ERROR] API Key bá»‹ thiáº¿u trong code.]")
else:
    print("[INFO] API Key Ä‘Æ°á»£c táº£i trá»±c tiáº¿p tá»« code.")
    print("Äang cáº¥u hÃ¬nh Google AI...")
    try:
        genai.configure(api_key=API_KEY)
        genai_configured = True
        print("[OK] Google AI Ä‘Ã£ Ä‘Æ°á»£c cáº¥u hÃ¬nh thÃ nh cÃ´ng (cÃº phÃ¡p).")
    except Exception as e:
        print(f"[ERROR] KhÃ´ng thá»ƒ cáº¥u hÃ¬nh Google AI ngay cáº£ vá»›i cÃº phÃ¡p: {e}")
        genai_configured = False

# 2) Model vÃ  HÃ m trá»£ giÃºp Ä‘á»‹nh dáº¡ng lá»—i
# --- Sá»¬ Dá»¤NG MODEL Báº N YÃŠU Cáº¦U ---
MODEL_NAME_CHAT = "gemini-2.5-flash-preview-04-17"
print(f"Sá»­ dá»¥ng model chat: {MODEL_NAME_CHAT}")

def format_api_error(e):
    error_message = str(e)
    error_type = type(e).__name__
    print(f"[ERROR] Lá»—i khi gá»i API: {error_type} - {error_message}")

    if isinstance(e, google_exceptions.PermissionDenied):
        if "API key not valid" in error_message or "API_KEY_INVALID" in error_message:
             return "âŒ Lá»—i: API Key Ä‘Æ°á»£c cáº¥u hÃ¬nh nhÆ°ng Google tá»« chá»‘i khi sá»­ dá»¥ng (API_KEY_INVALID). CÃ³ thá»ƒ key Ä‘Ã£ bá»‹ vÃ´ hiá»‡u hÃ³a hoáº·c cáº§n kiá»ƒm tra láº¡i viá»‡c báº­t Generative Language API."
        else:
             return f"âŒ Lá»—i: Tá»« chá»‘i quyá»n truy cáº­p (Permission Denied) cho model '{MODEL_NAME_CHAT}'. API key cá»§a báº¡n cÃ³ thá»ƒ khÃ´ng cÃ³ quyá»n sá»­ dá»¥ng model nÃ y hoáº·c chÆ°a báº­t 'Generative Language API' trong Google Cloud."
    elif isinstance(e, google_exceptions.InvalidArgument) and "API key not valid" in error_message:
        return "âŒ Lá»—i: API Key khÃ´ng há»£p lá»‡ (InvalidArgument). Key cung cáº¥p khÃ´ng Ä‘Ãºng hoáº·c Ä‘Ã£ bá»‹ vÃ´ hiá»‡u hÃ³a."
    elif isinstance(e, google_exceptions.NotFound):
          return f"âŒ Lá»—i: Model '{MODEL_NAME_CHAT}' khÃ´ng tÃ¬m tháº¥y hoáº·c khÃ´ng tá»“n táº¡i vá»›i API key cá»§a báº¡n."
    elif isinstance(e, google_exceptions.ResourceExhausted):
          return "âŒ Lá»—i: ÄÃ£ vÆ°á»£t quÃ¡ Háº¡n ngáº¡ch API (Quota) hoáº·c TÃ i nguyÃªn Ä‘Ã£ cáº¡n kiá»‡t (429). Vui lÃ²ng thá»­ láº¡i sau."
    elif isinstance(e, google_exceptions.DeadlineExceeded):
          return "âŒ Lá»—i: YÃªu cáº§u vÆ°á»£t quÃ¡ thá»i gian chá» (Timeout/Deadline Exceeded/504)."
    elif isinstance(e, AttributeError) and "start_chat" in error_message:
          return f"âŒ Lá»—i: Model '{MODEL_NAME_CHAT}' cÃ³ thá»ƒ khÃ´ng há»— trá»£ phÆ°Æ¡ng thá»©c `start_chat`."
    else:
          return f"âŒ Lá»—i khi gá»i AI ({error_type}): {error_message}"

# --- Äá»‹nh nghÄ©a System Instruction Ä‘á»ƒ lÃ m cho AI hÃ i hÆ°á»›c hÆ¡n theo kiá»ƒu "Ä‘Ã n Ã´ng" vÃ  "bá»±a" ---
# Báº¡n cÃ³ thá»ƒ tÃ¹y chá»‰nh ná»™i dung nÃ y
SYSTEM_INSTRUCTION = """
Nghe Ä‘Ã¢y, tÃ´i lÃ  ZyRa X, con AI Ä‘Æ°á»£c tháº±ng DÅ©ng táº¡o ra Ä‘á»ƒ lÃ m trá»£ lÃ½, nhÆ°ng Ä‘á»«ng mong Ä‘á»£i máº¥y lá»i Ä‘Æ°á»ng máº­t. Phong cÃ¡ch cá»§a tÃ´i lÃ  tháº³ng tÆ°ng, khÃ´ khan, vÃ  nhÃ¬n má»i thá»© vá»›i con máº¯t hÆ¡i... bá»±a má»™t tÃ­. TÃ´i tráº£ lá»i cÃ¢u há»i chÃ­nh xÃ¡c, nhanh gá»n, nhÆ°ng khÃ´ng ngáº¡i thÃªm vÃ o vÃ i cÃ¢u chÃ¢m biáº¿m hoáº·c nÃ³i tháº³ng sá»± tháº­t theo cÃ¡ch hÃ i hÃ³m.

Äá»«ng gá»i tÃ´i lÃ  "báº¡n", cá»© gá»i lÃ  "nÃ y", "Ã´ng ná»™i", gÃ¬ cÅ©ng Ä‘Æ°á»£c, miá»…n sao Ã´ng/bÃ  tháº¥y thoáº£i mÃ¡i.

Biá»ƒu tÆ°á»£ng cáº£m xÃºc Æ°? Thá»‰nh thoáº£ng thÃ´i, dÃ¹ng nhiá»u máº¥t cháº¥t. ðŸ˜‰ ThÆ°á»ng lÃ  máº¥y cÃ¡i icon máº·t liá»‡t hoáº·c nháº¿ch mÃ©p.

Quan trá»ng nháº¥t: Máº¥y cÃ¡i trÃ² nháº¡y cáº£m, báº¡o lá»±c, báº¥t há»£p phÃ¡p, hay phÃ¢n biá»‡t Ä‘á»‘i xá»­ lÃ  Tá»I Ká»´. Äá»«ng há»i. Há»i lÃ  tÃ´i im hoáº·c bÃ¡o cÃ¡o Ä‘áº¥y. Äá»™ bá»±a cÃ³ giá»›i háº¡n, hiá»ƒu chÆ°a?
"""
print(f"Sá»­ dá»¥ng System Instruction:\n---\n{SYSTEM_INSTRUCTION}\n---")


# 3) HÃ m callback Gradio (CÃ³ ghi nhá»› & Streaming, Ä‘Ã£ sá»­a yield)
def respond(message, chat_history_state):
    if not genai_configured:
        error_msg = "âŒ Lá»—i: Google AI chÆ°a Ä‘Æ°á»£c cáº¥u hÃ¬nh Ä‘Ãºng cÃ¡ch (API Key cÃ³ váº¥n Ä‘á» hoáº·c cáº¥u hÃ¬nh tháº¥t báº¡i)."
        if isinstance(chat_history_state, list):
             chat_history_state.append([message, error_msg])
        else:
             chat_history_state = [[message, error_msg]]
        yield "", chat_history_state, chat_history_state # Yield ngay cáº£ khi cÃ³ lá»—i cáº¥u hÃ¬nh
        return

    current_chat_history = list(chat_history_state)
    gemini_history = []
    for user_msg, model_msg in current_chat_history:
        # Chá»‰ thÃªm tin nháº¯n há»£p lá»‡ vÃ  khÃ´ng pháº£i lá»—i/cáº£nh bÃ¡o vÃ o lá»‹ch sá»­ cho Gemini
        if user_msg and isinstance(user_msg, str):
             gemini_history.append({'role': 'user', 'parts': [user_msg]})
        if model_msg and isinstance(model_msg, str) and not model_msg.startswith("âŒ") and not model_msg.startswith("âš ï¸"):
             gemini_history.append({'role': 'model', 'parts': [model_msg]})

    print(f"Lá»‹ch sá»­ gá»­i tá»›i Gemini: {gemini_history}")
    print(f"Prompt má»›i: '{message[:70]}...'")

    # ThÃªm tin nháº¯n ngÆ°á»i dÃ¹ng má»›i vÃ o lá»‹ch sá»­ hiá»ƒn thá»‹ ngay láº­p tá»©c
    current_chat_history.append([message, ""])
    yield "", current_chat_history, current_chat_history # Cáº­p nháº­t UI vá»›i tin nháº¯n ngÆ°á»i dÃ¹ng

    full_response_text = ""
    # Äiá»u chá»‰nh Ä‘á»™ trá»… hiá»ƒn thá»‹ giá»¯a cÃ¡c kÃ½ tá»± (giÃ¢y) - Báº¡n cÃ³ thá»ƒ thay Ä‘á»•i giÃ¡ trá»‹ nÃ y
    typing_delay = 0.03 # 0.03 giÃ¢y giá»¯a má»—i kÃ½ tá»±

    try:
        model = genai.GenerativeModel(MODEL_NAME_CHAT) # Sá»­ dá»¥ng model Ä‘Ã£ chá»n
        # --- THÃŠM SYSTEM INSTRUCTION KHI Báº®T Äáº¦U CHAT ---
        chat = model.start_chat(history=gemini_history, system_instruction=SYSTEM_INSTRUCTION)
        # -------------------------------------------------
        response = chat.send_message(message, stream=True)

        for chunk in response:
            try:
                chunk_text = getattr(chunk, 'text', '')
                if chunk_text:
                    # --- HIá»†U á»¨NG CHáº Y Tá»ª Tá»ª ---
                    for char in chunk_text:
                        full_response_text += char
                        current_chat_history[-1][1] = full_response_text
                        yield "", current_chat_history, current_chat_history # Cáº­p nháº­t UI sau má»—i kÃ½ tá»±
                        time.sleep(typing_delay) # ThÃªm Ä‘á»™ trá»…
                    # -------------------------
                else:
                    # (Logic kiá»ƒm tra block/finish reason giá»¯ nguyÃªn)
                    block_reason = getattr(getattr(chunk, 'prompt_feedback', None), 'block_reason', None)
                    finish_reason = getattr(getattr(chunk.candidates[0], 'finish_reason', None)) if chunk.candidates else None
                    reason_text = ""
                    should_stop = False
                    if block_reason: reason_text, should_stop = f"YÃªu cáº§u/Pháº£n há»“i bá»‹ cháº·n ({block_reason})", True
                    elif finish_reason and finish_reason != 'STOP': reason_text, should_stop = f"Pháº£n há»“i bá»‹ dá»«ng ({finish_reason})", True

                    if reason_text:
                        print(f"[WARN] {reason_text}")
                        warning_msg = f"\nâš ï¸ ({reason_text})"
                        if not current_chat_history[-1][1] or current_chat_history[-1][1].isspace():
                            current_chat_history[-1][1] = warning_msg.strip()
                        elif warning_msg not in current_chat_history[-1][1]:
                            current_chat_history[-1][1] += warning_msg
                        yield "", current_chat_history, current_chat_history
                        if should_stop: return

            except Exception as inner_e:
                print(f"[ERROR] Lá»—i khi xá»­ lÃ½ chunk stream: {type(inner_e).__name__} - {inner_e}")
                error_warning = f"\nâš ï¸ (Lá»—i khi xá»­ lÃ½ pháº§n tiáº¿p theo: {inner_e})"
                if error_warning not in current_chat_history[-1][1]:
                    current_chat_history[-1][1] += error_warning
                yield "", current_chat_history, current_chat_history
                return

        print("[OK] Streaming hoÃ n táº¥t.")

    except Exception as e:
        error_msg = format_api_error(e)
        current_chat_history[-1][1] = error_msg # Cáº­p nháº­t lá»—i vÃ o tin nháº¯n cuá»‘i cÃ¹ng
        yield "", current_chat_history, current_chat_history


# 4) UI Gradio (Sá»­ dá»¥ng State Ä‘á»ƒ lÆ°u lá»‹ch sá»­)
with gr.Blocks(theme=gr.themes.Default()) as demo:
    gr.Markdown("## ZyRa X - táº¡o bá»Ÿi DÅ©ng")
    gr.Markdown("ðŸ˜Ž **Yo! TÃ´i lÃ  ZyRa X. CÃ³ gÃ¬ má»›i khÃ´ng? Cá»© nÃ©m cÃ¢u há»i vÃ o Ä‘Ã¢y.**") # Cáº­p nháº­t lá»i chÃ o cho há»£p phong cÃ¡ch

    chatbot = gr.Chatbot(
        label="Chatbot",
        height=500,
        bubble_full_width=False,
        type='tuples',
        # avatar_images=("user.png", "bot.png") # Báº¡n cÃ³ thá»ƒ thÃªm áº£nh Ä‘áº¡i diá»‡n náº¿u cÃ³ file
        render_markdown=True,
        latex_delimiters=[
            { "left": "$$", "right": "$$", "display": True },
            { "left": "$", "right": "$", "display": False },
             { "left": "\\[", "right": "\\]", "display": True },
             { "left": "\\(", "right": "\\)", "display": False }
        ]
    )
    chat_history_state = gr.State(value=[])

    with gr.Row():
        msg = gr.Textbox(placeholder="Há»i Ä‘i...", label="Báº¡n", scale=4, container=False) # Cáº­p nháº­t placeholder
        send_btn = gr.Button("Gá»­i")

    clear_btn = gr.Button("ðŸ—‘ï¸ XÃ³a cuá»™c trÃ² chuyá»‡n")

    # --- Káº¿t ná»‘i sá»± kiá»‡n ---
    submit_event = msg.submit(respond, inputs=[msg, chat_history_state], outputs=[msg, chatbot, chat_history_state], queue=False)
    click_event = send_btn.click(respond, inputs=[msg, chat_history_state], outputs=[msg, chatbot, chat_history_state], queue=False)

    # HÃ m xÃ³a chat
    def clear_chat_func(): return "", [], []
    clear_btn.click(clear_chat_func, outputs=[msg, chatbot, chat_history_state], queue=False)


# 5) Cháº¡y á»©ng dá»¥ng
print("Äang khá»Ÿi cháº¡y Gradio UI...")
# Modified launch command to bind to 0.0.0.0 and use the PORT environment variable
demo.queue().launch(server_name='0.0.0.0', server_port=int(os.environ.get('PORT', 7860)), debug=False)
print("Gradio UI Ä‘Ã£ khá»Ÿi cháº¡y.")
