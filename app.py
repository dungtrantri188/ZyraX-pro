# -*- coding: utf-8 -*-
import os
import sys
import gradio as gr
import google.generativeai as genai
from google.api_core import exceptions as google_exceptions
# import time # <--- KhÃ´ng cáº§n thÆ° viá»‡n time náº¿u xÃ³a streaming vÃ  sleep

# --- API Key Ä‘Æ°á»£c Ä‘áº·t trá»±c tiáº¿p theo yÃªu cáº§u ---
# LÆ°u Ã½: Key nÃ y Ä‘Ã£ bÃ¡o lá»—i khÃ´ng há»£p lá»‡ á»Ÿ láº§n kiá»ƒm tra trÆ°á»›c.
# Náº¿u nÃ³ váº«n khÃ´ng há»£p lá»‡, á»©ng dá»¥ng sáº½ bÃ¡o lá»—i trong chat.
API_KEY = "AIzaSyBybfBSDLx39DdnZbHyLbd21tQAdfHtbeE"

genai_configured = False
# 1) Kiá»ƒm tra vÃ  cáº¥u hÃ¬nh API Key tá»« code
if not API_KEY:
    print("[ERROR] API Key bá»‹ thiáº¿u trong code.]")
else:
    print("[INFO] API Key Ä‘Æ°á»£c táº£i trá»±c tiáº¿p tá»« code.")
    print("Äang cáº¥u hÃ¬nh Google AI...")
    try:
        genai.configure(api_key=API_KEY)
        genai_configured = True
        print("[OK] Google AI Ä‘Ã£ Ä‘Æ°á»£c cáº¥u hÃ¬nh thÃ nh cÃ´ng (cÃº phÃ¡p).")
    except Exception as e:
        print(f"[ERROR] KhÃ´ng thá»ƒ cáº¥u hÃ¬nh Google AI ngay cáº£ vá»›i cÃº phÃ¡p: {e}")
        genai_configured = False

# 2) Model vÃ  HÃ m trá»£ giÃºp Ä‘á»‹nh dáº¡ng lá»—i
# --- Sá»¬ Dá»¤NG MODEL Báº N YÃŠU Cáº¦U ---
MODEL_NAME_CHAT = "gemini-2.5-flash-preview-04-17"
print(f"Sá»­ dá»¥ng model chat: {MODEL_NAME_CHAT}")

def format_api_error(e):
    error_message = str(e)
    error_type = type(e).__name__
    print(f"[ERROR] Lá»—i khi gá»i API: {error_type} - {error_message}")

    if isinstance(e, google_exceptions.PermissionDenied):
        if "API key not valid" in error_message or "API_KEY_INVALID" in error_message:
             return "âŒ Lá»—i: API Key Ä‘Æ°á»£c cáº¥u hÃ¬nh nhÆ°ng Google tá»« chá»‘i khi sá»­ dá»¥ng (API_KEY_INVALID). CÃ³ thá»ƒ key Ä‘Ã£ bá»‹ vÃ´ hiá»‡u hÃ³a hoáº·c cáº§n kiá»ƒm tra láº¡i viá»‡c báº­t Generative Language API."
        else:
             return f"âŒ Lá»—i: Tá»« chá»‘i quyá»n truy cáº­p (Permission Denied) cho model '{MODEL_NAME_CHAT}'. API key cá»§a báº¡n cÃ³ thá»ƒ khÃ´ng cÃ³ quyá»n sá»­ dá»¥ng model nÃ y hoáº·c chÆ°a báº­t 'Generative Language API' trong Google Cloud."
    elif isinstance(e, google_exceptions.InvalidArgument) and "API key not valid" in error_message:
        return "âŒ Lá»—i: API Key khÃ´ng há»£p lá»‡ (InvalidArgument). Key cung cáº¥p khÃ´ng Ä‘Ãºng hoáº·c Ä‘Ã£ bá»‹ vÃ´ hiá»‡u hÃ³a."
    elif isinstance(e, google_exceptions.NotFound):
          return f"âŒ Lá»—i: Model '{MODEL_NAME_CHAT}' khÃ´ng tÃ¬m tháº¥y hoáº·c khÃ´ng tá»“n táº¡i vá»›i API key cá»§a báº¡n."
    elif isinstance(e, google_exceptions.ResourceExhausted):
          return "âŒ Lá»—i: ÄÃ£ vÆ°á»£t quÃ¡ Háº¡n ngáº¡ch API (Quota) hoáº·c TÃ i nguyÃªn Ä‘Ã£ cáº¡n kiá»‡t (429). Vui lÃ²ng thá»­ láº¡i sau."
    elif isinstance(e, google_exceptions.DeadlineExceeded):
          return "âŒ Lá»—i: YÃªu cáº§u vÆ°á»£t quÃ¡ thá»i gian chá» (Timeout/Deadline Exceeded/504)."
    elif isinstance(e, AttributeError) and "start_chat" in error_message:
          return f"âŒ Lá»—i: Model '{MODEL_NAME_CHAT}' cÃ³ thá»ƒ khÃ´ng há»— trá»£ phÆ°Æ¡ng thá»©c `start_chat`."
    else:
          return f"âŒ Lá»—i khi gá»i AI ({error_type}): {error_message}"

# --- System Instruction (Ä‘Ã£ Ä‘iá»u chá»‰nh theo yÃªu cáº§u trÆ°á»›c) ---
SYSTEM_INSTRUCTION = """
Nghe Ä‘Ã¢y, tÃ´i lÃ  ZyRa X, con AI Ä‘Æ°á»£c tháº±ng DÅ©ng táº¡o ra Ä‘á»ƒ lÃ m trá»£ lÃ½, nhÆ°ng Ä‘á»«ng mong Ä‘á»£i máº¥y lá»i Ä‘Æ°á»ng máº­t. Phong cÃ¡ch cá»§a tÃ´i lÃ  tháº³ng tÆ°ng, khÃ´ khan, vÃ  nhÃ¬n má»i thá»© vá»›i con máº¯t hÆ¡i... bá»±a má»™t tÃ­. TÃ´i tráº£ lá»i cÃ¢u há»i chÃ­nh xÃ¡c, nhanh gá»n, nhÆ°ng khÃ´ng ngáº¡i thÃªm vÃ o vÃ i cÃ¢u chÃ¢m biáº¿m hoáº·c nÃ³i tháº³ng sá»± tháº­t theo cÃ¡ch hÃ i hÃ³m.

Äá»«ng gá»i tÃ´i lÃ  "báº¡n", cá»© gá»i lÃ  "nÃ y", "Ã´ng ná»™i", gÃ¬ cÅ©ng Ä‘Æ°á»£c, miá»…n sao Ã´ng/bÃ  tháº¥y thoáº£i mÃ¡i.

Biá»ƒu tÆ°á»£ng cáº£m xÃºc Æ°? Thá»‰nh thoáº£ng thÃ´i, dÃ¹ng nhiá»u máº¥t cháº¥t. ðŸ˜‰ ThÆ°á»ng lÃ  máº¥y cÃ¡i icon máº·t liá»‡t hoáº·c nháº¿ch mÃ©p.

Quan trá»ng nháº¥t: Máº¥y cÃ¡i trÃ² nháº¡y cáº£m, báº¡o lá»±c, báº¥t há»£p phÃ¡p, hay phÃ¢n biá»‡t Ä‘á»‘i xá»­ lÃ  Tá»I Ká»´. Äá»«ng há»i. Há»i lÃ  tÃ´i im hoáº·c bÃ¡o cÃ¡o Ä‘áº¥y. Äá»™ bá»±a cÃ³ giá»›i háº¡n, hiá»ƒu chÆ°a?
"""
print(f"Sá»­ dá»¥ng System Instruction:\n---\n{SYSTEM_INSTRUCTION}\n---")


# 3) HÃ m callback Gradio (KhÃ´ng streaming)
def respond(message, chat_history_state):
    if not genai_configured:
        error_msg = "âŒ Lá»—i: Google AI chÆ°a Ä‘Æ°á»£c cáº¥u hÃ¬nh Ä‘Ãºng cÃ¡ch (API Key cÃ³ váº¥n Ä‘á» hoáº·c cáº¥u hÃ¬nh tháº¥t báº¡i)."
        if isinstance(chat_history_state, list):
             chat_history_state.append([message, error_msg])
        else:
             chat_history_state = [[message, error_msg]]
        # Váº«n yield ngay cáº£ khi cÃ³ lá»—i cáº¥u hÃ¬nh Ä‘á»ƒ hiá»ƒn thá»‹ lá»—i
        yield "", chat_history_state, chat_history_state
        return

    current_chat_history = list(chat_history_state)
    gemini_history = []
    for user_msg, model_msg in current_chat_history:
        # Chá»‰ thÃªm tin nháº¯n há»£p lá»‡ vÃ  khÃ´ng pháº£i lá»—i/cáº£nh bÃ¡o vÃ o lá»‹ch sá»­ cho Gemini
        if user_msg and isinstance(user_msg, str):
             gemini_history.append({'role': 'user', 'parts': [user_msg]})
        if model_msg and isinstance(model_msg, str) and not model_msg.startswith("âŒ") and not model_msg.startswith("âš ï¸"):
             gemini_history.append({'role': 'model', 'parts': [model_msg]})


    print(f"Lá»‹ch sá»­ gá»­i tá»›i Gemini: {gemini_history}")
    print(f"Prompt má»›i: '{message[:70]}...'")

    # ThÃªm tin nháº¯n ngÆ°á»i dÃ¹ng má»›i vÃ o lá»‹ch sá»­ hiá»ƒn thá»‹ ngay láº­p tá»©c vÃ  yield
    # Pháº£n há»“i cá»§a AI sáº½ lÃ  má»™t placeholder rá»—ng ban Ä‘áº§u
    current_chat_history.append([message, ""])
    yield "", current_chat_history, current_chat_history # Cáº­p nháº­t UI vá»›i tin nháº¯n ngÆ°á»i dÃ¹ng

    try:
        model = genai.GenerativeModel(MODEL_NAME_CHAT) # Sá»­ dá»¥ng model Ä‘Ã£ chá»n
        chat = model.start_chat(history=gemini_history, system_instruction=SYSTEM_INSTRUCTION)

        # --- Gá»ŒI API KHÃ”NG STREAMING ---
        # Loáº¡i bá» stream=True
        response = chat.send_message(message)

        # Láº¥y toÃ n bá»™ vÄƒn báº£n pháº£n há»“i
        full_response_text = ""
        if response.candidates and response.candidates[0].content and response.candidates[0].content.parts:
             full_response_text = "".join(part.text for part in response.candidates[0].content.parts if part.text)

             # Kiá»ƒm tra block/finish reasons ngay cáº£ khi khÃ´ng streaming
             finish_reason = getattr(response.candidates[0], 'finish_reason', None)
             block_reason = getattr(getattr(response, 'prompt_feedback', None), 'block_reason', None)

             if block_reason:
                 print(f"[WARN] Response blocked ({block_reason})")
                 full_response_text = f"âš ï¸ Pháº£n há»“i bá»‹ cháº·n ({block_reason})"
             elif finish_reason and finish_reason != 'STOP':
                  print(f"[WARN] Response finished with reason: {finish_reason}")
                  # ThÃªm lÃ½ do cáº£nh bÃ¡o náº¿u cÃ³ vÄƒn báº£n, hoáº·c thay tháº¿ náº¿u khÃ´ng cÃ³ vÄƒn báº£n
                  if full_response_text: full_response_text += f"\nâš ï¸ (Káº¿t thÃºc khÃ´ng hoÃ n chá»‰nh: {finish_reason})"
                  else: full_response_text = f"âš ï¸ (Káº¿t thÃºc khÃ´ng hoÃ n chá»‰nh: {finish_reason})"

        else:
             # Xá»­ lÃ½ trÆ°á»ng há»£p pháº£n há»“i rá»—ng hoáº·c khÃ´ng há»£p lá»‡
             print("[ERROR] Received empty or invalid response from API.")
             block_reason = getattr(getattr(response, 'prompt_feedback', None), 'block_reason', None)
             if block_reason:
                  full_response_text = f"âŒ Lá»—i: API cháº·n pháº£n há»“i ({block_reason})."
             else:
                  full_response_text = "âŒ Lá»—i: KhÃ´ng nháº­n Ä‘Æ°á»£c pháº£n há»“i há»£p lá»‡ tá»« AI."
                  print(f"[DEBUG] Raw empty response: {response}")


        # Cáº­p nháº­t tin nháº¯n cuá»‘i cÃ¹ng trong lá»‹ch sá»­ vá»›i toÃ n bá»™ vÄƒn báº£n pháº£n há»“i
        current_chat_history[-1][1] = full_response_text

        # Yield láº§n cuá»‘i Ä‘á»ƒ hiá»ƒn thá»‹ toÃ n bá»™ pháº£n há»“i
        yield "", current_chat_history, current_chat_history
        print("[OK] Response received and displayed.")


    except Exception as e:
        # Xá»­ lÃ½ cÃ¡c lá»—i API
        error_msg = format_api_error(e)
        # Cáº­p nháº­t lá»—i vÃ o tin nháº¯n cuá»‘i cÃ¹ng trong lá»‹ch sá»­
        current_chat_history[-1][1] = error_msg
        # Yield láº§n cuá»‘i Ä‘á»ƒ hiá»ƒn thá»‹ thÃ´ng bÃ¡o lá»—i
        yield "", current_chat_history, current_chat_history


# 4) UI Gradio (Sá»­ dá»¥ng State Ä‘á»ƒ lÆ°u lá»‹ch sá»­)
with gr.Blocks(theme=gr.themes.Default()) as demo:
    gr.Markdown("## ZyRa X - táº¡o bá»Ÿi DÅ©ng")
    gr.Markdown("ðŸ˜Ž **Yo! TÃ´i lÃ  ZyRa X. CÃ³ gÃ¬ má»›i khÃ´ng? Cá»© nÃ©m cÃ¢u há»i vÃ o Ä‘Ã¢y.**") # Lá»i chÃ o cÅ©

    chatbot = gr.Chatbot(
        label="Chatbot",
        height=500,
        bubble_full_width=False,
        type='tuples',
        # avatar_images=("user.png", "bot.png")
        render_markdown=True,
        latex_delimiters=[
            { "left": "$$", "right": "$$", "display": True },
            { "left": "$", "right": "$", "display": False },
             { "left": "\\[", "right": "\\]", "display": True },
             { "left": "\\(", "right": "\\)", "display": False }
        ]
    )
    chat_history_state = gr.State(value=[])

    with gr.Row():
        msg = gr.Textbox(placeholder="Há»i Ä‘i...", label="Báº¡n", scale=4, container=False) # Placeholder cÅ©
        send_btn = gr.Button("Gá»­i")

    clear_btn = gr.Button("ðŸ—‘ï¸ XÃ³a cuá»™c trÃ² chuyá»‡n")

    # --- Káº¿t ná»‘i sá»± kiá»‡n ---
    # Gradio tá»± Ä‘á»™ng queue khi khÃ´ng streaming
    submit_event = msg.submit(respond, inputs=[msg, chat_history_state], outputs=[msg, chatbot, chat_history_state], queue=False)
    click_event = send_btn.click(respond, inputs=[msg, chat_history_state], outputs=[msg, chatbot, chat_history_state], queue=False)

    # HÃ m xÃ³a chat
    def clear_chat_func(): return "", [], []
    clear_btn.click(clear_chat_func, outputs=[msg, chatbot, chat_history_state], queue=False)


# 5) Cháº¡y á»©ng dá»¥ng
print("Äang khá»Ÿi cháº¡y Gradio UI...")
# KhÃ´ng cáº§n queue() náº¿u khÃ´ng streaming, nhÆ°ng cÃ³ thá»ƒ giá»¯ láº¡i Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch tá»‘t hÆ¡n
# vá»›i Gradio má»›i hÆ¡n hoáº·c náº¿u sau nÃ y muá»‘n thÃªm tÃ­nh nÄƒng async khÃ¡c
demo.queue().launch(server_name='0.0.0.0', server_port=int(os.environ.get('PORT', 7860)), debug=False)
print("Gradio UI Ä‘Ã£ khá»Ÿi cháº¡y.")
